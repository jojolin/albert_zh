{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOST_PORT_HTTP = \"your-host-port\" # TODO: e.g. 123.123.123.123:8511\n",
    "MODEL_NAME = \"albert_remy_lac\" # TODO: your model name\n",
    "VOCAB_FILE = '/data1/albert/zh/albert_base_zh/vocab.txt' # TODO: set vocab.txt path\n",
    "LABEL_CLASS_FILE = './label_class.txt' # TODO: your label_class.txt\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ***********************************\n",
    "## tensorflow-serving-api REST\n",
    "## ***********************************\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def predict(input_ids, input_mask, segment_ids, label_ids, hostport_http, version=\"1\"):\n",
    "    data = json.dumps({\n",
    "            \"signature_name\": \"serving_default\", \n",
    "            \"instances\": [{\n",
    "                \"input_ids\": input_ids,\n",
    "                \"input_mask\": input_mask,\n",
    "                \"label_ids\": label_ids,\n",
    "                \"segment_ids\": segment_ids  \n",
    "            }],\n",
    "        })\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    url = 'http://{}/v1/models/{}/versions/{}:predict'.format(hostport_http, MODEL_NAME, version)\n",
    "    #print(url)\n",
    "    json_response = requests.post(url, data=data, headers=headers)\n",
    "    #print(json_response.text)\n",
    "    predictions = json.loads(json_response.text)['predictions']\n",
    "    return [np.argmax(x) for pred in predictions for x in pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "    def __init__(self, text_a, label):\n",
    "        self.text_a = text_a\n",
    "        self.label = label\n",
    "    \n",
    "def convert_single_example(example, label_map, max_seq_length, tokenizer):\n",
    "    textlist = list(example.text_a)\n",
    "    labellist = example.label.split(' ')\n",
    "   \n",
    "    if len(textlist) != len(labellist):\n",
    "        print(textlist, labellist)\n",
    "        print(len(textlist), len(labellist))\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i, word in enumerate(textlist):\n",
    "        token = tokenizer.tokenize(word)\n",
    "        tokens.extend(token)\n",
    "        label_1 = labellist[i]\n",
    "        for m in range(len(token)):\n",
    "            if m == 0:\n",
    "                labels.append(label_1)\n",
    "            else:\n",
    "                labels.append(\"X\")\n",
    "\n",
    "    if len(tokens) >= max_seq_length - 1:\n",
    "        tokens = tokens[0:(max_seq_length - 2)]\n",
    "        labels = labels[0:(max_seq_length - 2)]\n",
    "\n",
    "    ntokens = []\n",
    "    nlabels = []\n",
    "    segment_ids = []\n",
    "    label_ids = []\n",
    "\n",
    "    ntokens.append(\"[CLS]\")\n",
    "    nlabels.append(\"[CLS]\")\n",
    "    label_ids.append(label_map[\"[CLS]\"]) # append(\"O\") or append(\"[CLS]\") not sure!\n",
    "    segment_ids.append(0)\n",
    "    for i, token in enumerate(tokens):\n",
    "        ntokens.append(token)\n",
    "        nlabels.append(labels[i])\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(label_map[labels[i]])\n",
    "\n",
    "    ntokens.append(\"[SEP]\")\n",
    "    nlabels.append(\"[SEP]\")\n",
    "    label_ids.append(label_map[\"[SEP]\"])\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    #label_mask = [1] * len(input_ids)\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(0) # we don't concerned about it!\n",
    "        ntokens.append(\"**NULL**\")\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tokenization\n",
    "import merge_lac\n",
    "\n",
    "def get_labels(label_class_file):\n",
    "    with open(label_class_file, 'r', encoding='utf8') as r:\n",
    "        return [x.strip() for x in r.readlines()]\n",
    "    \n",
    "# 获取tokenizer\n",
    "do_lower_case = True\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=do_lower_case)\n",
    "\n",
    "# 构造 label_id_map 和 id_label_map\n",
    "label_list = get_labels(LABEL_CLASS_FILE)\n",
    "label_id_map = {}\n",
    "id_label_map = {}\n",
    "for (i, label) in enumerate(label_list, 1):\n",
    "    label_id_map[label] = i\n",
    "    id_label_map[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['查/v', '一下/m', '餐/nz', '包/n', '有/v', '哪些/r', '做法/n']\n",
      "['瘦肉末/nz', '或者/c', '猪肉丸/nz', '能/v', '做/v', '什么/r']\n",
      "['猪肉/n', '丁/nz', '可以/v', '和/p', '什么/r', '一起/d', '煮汤/vn']\n",
      "['看看/v', '蒜/n', '香/a', '排骨/v', '的/u', '做法/n']\n",
      "['如何/d', '调理/v', '不/d', '正常/a', '的/u', '日常/a', '膳食/n']\n",
      "['能/v', '介绍/v', '些/q', '养颜/vn', '美容/vn', '的/u', '食物/n', '吗/xc']\n",
      "['健康/a', '饮食：/n', '怎样/d', '吃/v', '海鲜/n', '更/d', '安全/a']\n",
      "['吃/v', '大闸蟹/nz', '应该/v', '注意/v', '什么/r']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    '查一下餐包有哪些做法',\n",
    "    '瘦肉末或者猪肉丸能做什么',\n",
    "    '猪肉丁可以和什么一起煮汤',\n",
    "    '看看蒜香排骨的做法',\n",
    "    '如何调理不正常的日常膳食',\n",
    "    '能介绍些养颜美容的食物吗',\n",
    "    '健康饮食：怎样吃海鲜更安全',\n",
    "    '吃大闸蟹应该注意什么',\n",
    "    ]\n",
    "\n",
    "# 预测结果 [CLS] ...\n",
    "def predict_with_version(version):\n",
    "    for text in texts:\n",
    "        example = Example(text, ' '.join(['O'] * len(text)))\n",
    "        # 转换为模型需要的example\n",
    "        input_ids, input_mask, segment_ids, label_ids = convert_single_example(example, label_id_map, MAX_SEQ_LENGTH, tokenizer)\n",
    "        #print(input_ids)\n",
    "\n",
    "        predict_label_ids = predict(input_ids, input_mask, segment_ids, label_ids, HOST_PORT_HTTP, version)\n",
    "\n",
    "        # predict_label_ids[1:] 去掉 CLS 的词性\n",
    "        predict_labels = [id_label_map[labelid] for labelid in predict_label_ids[1:] if labelid != 0]\n",
    "\n",
    "        print(merge_lac.merge_line2(list(text), predict_labels))\n",
    "\n",
    "predict_with_version('1574843457') # TODO: repalce the version with yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
